{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMli3phtQooa1dJIyMtwSbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atikjain55/Automatic-Resume-Evaluation-System/blob/main/NER_using_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwGrtfO24pRo"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('./NLP/'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5JKaMhK43Tr"
      },
      "source": [
        "# import logging\n",
        "import json\n",
        "import re\n",
        "\n",
        "# JSON formatting functions\n",
        "def convert_dataturks_to_spacy(json_file_path):\n",
        "    training_data = []\n",
        "    lines=[]\n",
        "    with open(json_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        if line==\"\":\n",
        "            print(\"empty line\")\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        entities = []\n",
        "        data_annotations = data['annotation']\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    point_start = point['start']\n",
        "                    point_end = point['end']\n",
        "                    point_text = point['text']\n",
        "\n",
        "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                    if lstrip_diff != 0:\n",
        "                        point_start = point_start + lstrip_diff\n",
        "                    if rstrip_diff != 0:\n",
        "                        point_end = point_end - rstrip_diff\n",
        "                    entities.append((point_start, point_end + 1 , label))\n",
        "        training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    return cleaned_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9JNEZNjf47Nm",
        "outputId": "d8e0c4a2-3c77-4dea-96ef-0e4859d9b3d3"
      },
      "source": [
        "data = trim_entity_spans(convert_dataturks_to_spacy(\"./NLP/Entity Recognition in Resumes.json\"))\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-15cf45067ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_entity_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_dataturks_to_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./NLP/Entity Recognition in Resumes.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a462f7c91e1e>\u001b[0m in \u001b[0;36mconvert_dataturks_to_spacy\u001b[0;34m(json_file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './NLP/Entity Recognition in Resumes.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFpMjZy94_nX"
      },
      "source": [
        "def clean_entities(training_data):\n",
        "    \n",
        "     clean_data = []\n",
        "     for text, annotation in training_data:\n",
        "        \n",
        "         entities = annotation.get('entities')\n",
        "         entities_copy = entities.copy()\n",
        "       \n",
        "         # append entity only if it is longer than its overlapping entity\n",
        "         i = 0\n",
        "         for entity in entities_copy:\n",
        "             j = 0\n",
        "             for overlapping_entity in entities_copy:\n",
        "                 # Skip self\n",
        "                 if i != j:\n",
        "                     e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n",
        "                     # Delete any entity that overlaps, keep if longer\n",
        "                     if ((e_start >= oe_start and e_start <= oe_end) \\\n",
        "                     or (e_end <= oe_end and e_end >= oe_start)) \\\n",
        "                     and ((e_end - e_start) <= (oe_end - oe_start)):\n",
        "                         entities.remove(entity)\n",
        "                 j += 1\n",
        "             i += 1\n",
        "         clean_data.append((text, {'entities': entities}))\n",
        "               \n",
        "     return clean_data\n",
        "data = clean_entities(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv0pX-5GQh"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def train_test_split(data, test_size, random_state):\n",
        "\n",
        "    random.Random(random_state).shuffle(data)\n",
        "    test_idx = len(data) - math.floor(test_size * len(data))\n",
        "    train_set = data[0: test_idx]\n",
        "    test_set = data[test_idx: ]\n",
        "\n",
        "    return train_set, test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgO4eE4j5M8v"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size = 0.1, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmSqC36-5PgW"
      },
      "source": [
        "def train_spacy():\n",
        "    \n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "      \n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "         for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "          \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(10):\n",
        "            print(\"Starting iteration \" + str(itn))\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            for text, annotations in train_data:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    return nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_sf30en5R4Z",
        "outputId": "70d8360e-1eaf-4bd6-eb8e-1276a56429c6"
      },
      "source": [
        "nlp = train_spacy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting iteration 0\n",
            "{'ner': 22286.373561714707}\n",
            "Starting iteration 1\n",
            "{'ner': 17487.633669415176}\n",
            "Starting iteration 2\n",
            "{'ner': 13402.510354205631}\n",
            "Starting iteration 3\n",
            "{'ner': 13303.678709161606}\n",
            "Starting iteration 4\n",
            "{'ner': 13300.484113648534}\n",
            "Starting iteration 5\n",
            "{'ner': 12342.20715512839}\n",
            "Starting iteration 6\n",
            "{'ner': 10887.051619787746}\n",
            "Starting iteration 7\n",
            "{'ner': 9454.769237496577}\n",
            "Starting iteration 8\n",
            "{'ner': 10497.795937107152}\n",
            "Starting iteration 9\n",
            "{'ner': 9891.889112478688}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiEBC4vI5T-n",
        "outputId": "0b6fd22b-4dd0-4176-b0b2-c7f369196b86"
      },
      "source": [
        "from spacy.gold import GoldParse\n",
        "from itertools import groupby\n",
        "\n",
        "def doc_to_bilou(nlp, text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
        "    entities = []\n",
        "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
        "        if not entity:\n",
        "            continue\n",
        "        group = list(group)\n",
        "        _, start, _ = group[0]\n",
        "        word, last, _ = group[-1]\n",
        "        end = last + len(word)\n",
        "        \n",
        "        entities.append((\n",
        "                start,\n",
        "                end,\n",
        "                entity\n",
        "            ))\n",
        "\n",
        "    gold = GoldParse(nlp(text), entities = entities)\n",
        "    pred_ents = gold.ner\n",
        "    \n",
        "    return pred_ents\n",
        "\n",
        "y_test = []\n",
        "y_pred = []\n",
        "\n",
        "for text, annots in test_data:\n",
        "    \n",
        "    gold = GoldParse(nlp.make_doc(text), entities = annots.get(\"entities\"))\n",
        "    ents = gold.ner\n",
        "    pred_ents = doc_to_bilou(nlp, text)\n",
        "    \n",
        "    y_test.append(ents)\n",
        "    y_pred.append(pred_ents)\n",
        "    \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from itertools import chain\n",
        "\n",
        "def ner_report(y_true, y_pred):\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_)\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset\n",
        "    ), accuracy_score(y_true_combined, y_pred_combined)\n",
        "    \n",
        "report, accuracy = ner_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                    -       0.00      0.00      0.00       142\n",
            "       B-College Name       0.65      0.81      0.72        32\n",
            "       I-College Name       0.60      0.86      0.71        63\n",
            "       L-College Name       0.62      0.78      0.69        32\n",
            "       U-College Name       1.00      1.00      1.00         1\n",
            "B-Companies worked at       0.59      0.67      0.62        30\n",
            "I-Companies worked at       0.00      0.25      0.01         4\n",
            "L-Companies worked at       0.53      0.60      0.56        30\n",
            "U-Companies worked at       0.32      0.27      0.29        41\n",
            "             B-Degree       0.91      0.83      0.87        24\n",
            "             I-Degree       0.88      0.92      0.90        66\n",
            "             L-Degree       0.91      0.83      0.87        24\n",
            "             U-Degree       0.20      0.67      0.31         3\n",
            "        B-Designation       0.65      0.68      0.67        47\n",
            "        I-Designation       0.82      0.57      0.68        40\n",
            "        L-Designation       0.63      0.66      0.65        47\n",
            "        U-Designation       0.00      0.00      0.00         1\n",
            "      B-Email Address       1.00      0.86      0.92         7\n",
            "      L-Email Address       1.00      0.86      0.92         7\n",
            "      U-Email Address       0.62      1.00      0.77        10\n",
            "    U-Graduation Year       0.44      0.73      0.55        22\n",
            "           B-Location       1.00      0.33      0.50         3\n",
            "           L-Location       1.00      0.33      0.50         3\n",
            "           U-Location       0.47      0.66      0.55        32\n",
            "               B-Name       0.95      0.91      0.93        23\n",
            "               L-Name       0.95      0.91      0.93        23\n",
            "                    O       0.95      0.95      0.95     12457\n",
            "             B-Skills       0.71      0.63      0.67        27\n",
            "             I-Skills       0.82      0.57      0.68      1022\n",
            "             L-Skills       0.71      0.63      0.67        27\n",
            "             U-Skills       0.00      0.00      0.00         2\n",
            "B-Years of Experience       0.00      0.00      0.00         4\n",
            "L-Years of Experience       0.00      0.00      0.00         4\n",
            "U-Years of Experience       0.00      0.00      0.00         1\n",
            "\n",
            "            micro avg       0.90      0.90      0.90     14301\n",
            "            macro avg       0.59      0.58      0.56     14301\n",
            "         weighted avg       0.92      0.90      0.91     14301\n",
            "          samples avg       0.90      0.90      0.90     14301\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJjE5jJ37S6l",
        "outputId": "a67a1d28-78f0-4c1c-8c78-2a0253c1ba6a"
      },
      "source": [
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9011957205789805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhoQk_cc7Wft"
      },
      "source": [
        "nlp.to_disk('nlp_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSIxjcWo7YM8"
      },
      "source": [
        "nlp_model = spacy.load('nlp_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIZz5Kvm7bNO"
      },
      "source": [
        "text = train_data[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1Snj5Qi7c31",
        "outputId": "ef7d88aa-5cb8-422d-8d48-b5fcf5d7ae93"
      },
      "source": [
        "doc = nlp_model(text)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME                          - Sridevi H\n",
            "LOCATION                      - Bangalore\n",
            "EMAIL ADDRESS                 - indeed.com/r/Sridevi-H/63703b24aaaa54e4\n",
            "DESIGNATION                   - Principal System Engineer\n",
            "COMPANIES WORKED AT           - Aricent Technologies\n",
            "DESIGNATION                   - Technical Lead\n",
            "DEGREE                        - M.S in Software Systems\n",
            "COLLEGE NAME                  - BITS Pilani\n",
            "LOCATION                      - Pilani\n",
            "DEGREE                        - B.E. in Computer Science\n",
            "COLLEGE NAME                  - Board of Technical Education\n",
            "SKILLS                        - Networking/Platform/Drivers/Vxworks\n",
            "GRADUATION YEAR               - 2016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXJ9cPkr7epO",
        "outputId": "67e4dc95-9b37-4b6b-ccce-753f5fa3ba2c"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.7/dist-packages (1.18.15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0cS1SW67gyH"
      },
      "source": [
        "import sys, fitz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qwst_SJ7i4O"
      },
      "source": [
        "fname = 'Alice Clark CV.pdf'\n",
        "doc = fitz.open(fname)\n",
        "text = \"\"\n",
        "\n",
        "for page in doc:\n",
        "    text = text + str(page.getText())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bvphukP7pGt"
      },
      "source": [
        "tx = \" \".join(text.split('\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "uqRvWtcs7q62",
        "outputId": "dc1b1e57-4133-4c6a-bdd3-1bababcc017c"
      },
      "source": [
        "tx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Alice Clark  AI / Machine Learning    Delhi, India Email me on Indeed  •  20+ years of experience in data handling, design, and development  •  Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to  data warehousing and business intelligence  •  Database: Experience in database designing, scalability, back-up and recovery, writing and  optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes.  Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure,  Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake  analytics(U-SQL)  Willing to relocate anywhere    WORK EXPERIENCE  Software Engineer  Microsoft – Bangalore, Karnataka  January 2000 to Present  1. Microsoft Rewards Live dashboards:  Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping  online. Microsoft Rewards members can earn points when searching with Bing, browsing with  Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft  Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft  rewards website. Rewards live dashboards gives a live picture of usage world-wide and by  markets like US, Canada, Australia, new user registration count, top/bottom performing rewards  offers, orders stats and weekly trends of user activities, orders and new user registrations. the  PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.  Technology/Tools used    EDUCATION  Indian Institute of Technology – Mumbai  2001    SKILLS  Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ6Pnh-Q7sR-",
        "outputId": "6ff708b9-8f87-40e4-8704-596aa1908fd0"
      },
      "source": [
        "doc = nlp_model(tx)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME                          - Alice Clark\n",
            "LOCATION                      - Delhi\n",
            "EMAIL ADDRESS                 - •\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "DESIGNATION                   - Software Engineer\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "LOCATION                      - Bangalore\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "COMPANIES WORKED AT           - Microsoft\n",
            "LOCATION                      - Store\n",
            "DEGREE                        - EDUCATION\n",
            "COLLEGE NAME                  - Indian Institute of Technology – Mumbai\n",
            "GRADUATION YEAR               - 2001\n",
            "SKILLS                        - Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bTYI3Hv7uaA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}